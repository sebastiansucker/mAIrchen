# üìö mAIrchen - Geschichten f√ºr Kinder

Eine M√§rchen-Schreib-App f√ºr Grundschulkinder (Klasse 1-4), die personalisierte Geschichten mit W√∂rtern aus dem Grundwortschatz generiert.

## ‚ú® Funktionen

- **Personalisierte Geschichten**: Der Nutzer gibt Thema, Personen/Tiere, Ort und Stimmung ein
- **Zufalls-Generator**: Automatische Vorschl√§ge f√ºr alle Parameter
- **Grundwortschatz-Integration**: Geschichten enthalten W√∂rter aus dem Grundwortschatz der Klassen 1-4
- **Buchlayout**: Ansprechende Darstellung im Buchformat mit vergilbtem Papier-Look
- **Seitenbl√§tter-Animation**: Geschichten erscheinen mit einer 3D-Bl√§tter-Animation
- **Flexible AI-Provider**: Unterst√ºtzt Mistral, Ollama Cloud und lokale Ollama-Instanzen
- **Missbrauchsschutz**: Rate Limiting, Cost Control und Request-Validierung ohne Login
- **Single-Container**: Frontend und Backend in einem Container f√ºr einfaches Deployment

## üöÄ Installation

### Voraussetzungen

- Docker und Docker Compose
- AI Provider API Key (Mistral oder Ollama Cloud) ODER lokale Ollama Installation

### Setup

1. Repository klonen und in das Verzeichnis wechseln:
```bash
cd mAIrchen
```

2. Umgebungsvariablen konfigurieren:
```bash
cp .env.example .env
```

3. `.env`-Datei bearbeiten und AI Provider konfigurieren:

**Option A: Mistral (Standard)**
```env
AI_PROVIDER=mistral
MISTRAL_API_KEY=your-mistral-api-key
```

**Option B: Ollama Cloud**
```env
AI_PROVIDER=ollama-cloud
OLLAMA_API_KEY=your-ollama-api-key
OLLAMA_MODEL=llama3.2:3b
```

**Option C: Ollama Lokal (kostenlos)** ‚≠ê Empfohlen f√ºr Entwicklung
```env
AI_PROVIDER=ollama-local
OLLAMA_BASE_URL=http://host.docker.internal:11434/v1
OLLAMA_MODEL=gemma3:latest  # Beste Balance: Schnell & gute Qualit√§t
```

**Empfohlene Ollama-Modelle f√ºr Kindergeschichten:**
- `gemma3:latest` - üèÜ Beste Wahl (7.5s, sehr gute Qualit√§t)
- `gemma3n:latest` - Etwas langsamer, exzellente Qualit√§t (14.9s)
- `llama3.2:3b` - Klein und schnell, gute Basisqualit√§t
- `gemma3:27b` - Beste Qualit√§t, aber langsam (38s)

Siehe [OLLAMA.md](OLLAMA.md) f√ºr detaillierte Ollama-Konfiguration und vollst√§ndigen Modell-Vergleich.

4. Container bauen & starten:
```bash
docker-compose --env-file .env -f docker/docker-compose.yml build
docker-compose --env-file .env -f docker/docker-compose.yml up -d
```

Die App ist nun verf√ºgbar unter:
- **Frontend**: http://localhost
- **API Info**: http://localhost/api (zeigt aktiven Provider & Modell)
- **API Stats**: http://localhost/api/stats (Monitoring)
- **Health Check**: http://localhost/health

## üìä API Endpoints

Alle API-Endpoints sind √ºber Port 80 (HTTP) erreichbar:

### GET /api
Zeigt aktiven AI-Provider und Modell:
```bash
curl http://localhost/api
```
```json
{
  "message": "mAIrchen API - M√§rchen f√ºr Kinder",
  "ai_provider": "ollama-local",
  "model": "gemma3:latest"
}
```

### GET /api/stats
Monitoring und Nutzungsstatistiken:
```bash
curl http://localhost/api/stats
```
```json
{
  "global_requests_today": 42,
  "global_limit": 1000,
  "estimated_cost_today": 0.0,
  "daily_budget": 5.0,
  "budget_remaining": 5.0,
  "rate_limit_per_ip": 10,
  "active_ips": 8
}
```

### POST /api/generate-story
Generiert eine personalisierte Geschichte:
```bash
curl -X POST http://localhost/api/generate-story \
  -H "Content-Type: application/json" \
  -d '{
    "thema": "Freundschaft",
    "personen_tiere": "Ein kleiner Igel",
    "ort": "im Wald",
    "stimmung": "herzlich",
    "laenge": 3,
    "klassenstufe": "34"
  }'
```

### GET /api/random
Zuf√§llige Vorschl√§ge f√ºr alle Parameter:
```bash
curl http://localhost/api/random
```

## üîí Sicherheit & Missbrauchsschutz

Die API ist √ºber Port 80 erreichbar, aber durch mehrere Schutzebenen gesichert:

### Aktive Schutzma√ünahmen:
- **Rate Limiting**: 10 Anfragen/h pro IP, 1000/Tag global (IP-basiert)
- **Request-Validierung**: Max 15 Min Story-L√§nge, 200 Zeichen pro Feld
- **Cost Control**: Max 5‚Ç¨/Tag Budget mit automatischem Stop
- **CORS-Schutz**: Nur erlaubte Origins (konfigurierbar)
- **Nginx Reverse Proxy**: Backend nur intern erreichbar (127.0.0.1:8000)
- Keine User-Accounts erforderlich - Privacy-Friendly!

### Wie es funktioniert:
1. Backend l√§uft nur auf `127.0.0.1:8000` (nicht von au√üen erreichbar)
2. Nginx auf Port 80 leitet Anfragen an Backend weiter
3. Rate Limiting pr√ºft jede Anfrage anhand der IP-Adresse
4. CORS verhindert Zugriff von fremden Websites

Details: [SECURITY.md](SECURITY.md)

## üèóÔ∏è Architektur

### Single Container Setup
Frontend und Backend laufen in einem Docker-Container:
- **Nginx** serviert das Frontend (Port 80)
- **FastAPI** Backend l√§uft auf Port 8000 (intern)
- Nginx fungiert als Reverse Proxy f√ºr `/api/*` Requests

### Backend (FastAPI)
- Python-basierte REST API
- OpenAI-kompatibler Client (unterst√ºtzt Mistral, Ollama Cloud, Ollama Local)
- Flexible AI-Provider-Konfiguration √ºber Umgebungsvariablen
- Grundwortschatz-Integration aus `backend/gws.md`
- Rate Limiting & Cost Tracking
- Endpunkte:
  - `GET /` - API Info (Provider & Modell)
  - `GET /api/random` - Zuf√§llige Vorschl√§ge
  - `POST /api/generate-story` - Geschichte generieren
  - `GET /api/stats` - Monitoring & Statistiken
  - `GET /health` - Health Check

### Frontend
- Vanilla HTML/CSS/JavaScript
- Responsive Design
- 3D-Seitenbl√§tter-Animation
- Buchlayout mit vergilbtem Papier-Effekt
- Automatische API-URL-Erkennung (funktioniert im Netzwerk)

### Projektstruktur
```
mAIrchen/
‚îú‚îÄ‚îÄ .env.example          # Umgebungsvariablen Template
‚îú‚îÄ‚îÄ .gitignore           # Git Ignore Datei
‚îú‚îÄ‚îÄ README.md            # Diese Datei
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ main.py          # FastAPI Backend
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt # Python Dependencies
‚îÇ   ‚îî‚îÄ‚îÄ gws.md          # Grundwortschatz Klassen 1-4
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ index.html      # Haupt-HTML
‚îÇ   ‚îú‚îÄ‚îÄ styles.css      # Styling & Animationen
‚îÇ   ‚îú‚îÄ‚îÄ app.js          # JavaScript Logik
‚îÇ   ‚îú‚îÄ‚îÄ logo.png        # App Logo (transparent)
‚îÇ   ‚îî‚îÄ‚îÄ app_icon.png    # App Icon
‚îî‚îÄ‚îÄ docker/
    ‚îú‚îÄ‚îÄ Dockerfile              # Multi-Stage Build
    ‚îú‚îÄ‚îÄ docker-compose.yml      # Container Orchestrierung
    ‚îî‚îÄ‚îÄ nginx-combined.conf     # Nginx Konfiguration
```

## üéØ Verwendung

1. App im Browser √∂ffnen (http://localhost)
2. Eingabefelder ausf√ºllen:
   - Thema (z.B. "Freundschaft")
   - Personen/Tiere (z.B. "Ein kleiner Hase")
   - Ort (z.B. "im Wald")
   - Stimmung (z.B. "fr√∂hlich")
3. Optional: "üé≤ Zuf√§llig" Button f√ºr automatische Vorschl√§ge
4. "‚ú® Geschichte erstellen" klicken
5. Geschichte im Buchlayout lesen

## üõ†Ô∏è Entwicklung

### Backend lokal starten
```bash
cd backend
pip install -r requirements.txt
export MISTRAL_API_KEY=your-key
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Frontend lokal testen
Das Frontend ben√∂tigt das Backend auf Port 8000:
```bash
cd frontend
python -m http.server 8080
```
Dann im Browser: http://localhost:8080

### Container neu bauen nach √Ñnderungen
```bash
docker-compose --env-file .env -f docker/docker-compose.yml build
docker-compose --env-file .env -f docker/docker-compose.yml up -d
```

## üìù API Endpunkte

### Zuf√§llige Vorschl√§ge
```http
GET /api/random
```

### Geschichte generieren
```http
POST /api/generate-story
Content-Type: application/json

{
  "thema": "Abenteuer",
  "personen_tiere": "Ein mutiger Fuchs",
  "ort": "im Wald",
  "stimmung": "spannend"
}
```

## üîß Konfiguration

Umgebungsvariablen in `.env`:

### AI Provider
- `AI_PROVIDER`: `mistral`, `ollama-cloud` oder `ollama-local` (Standard: mistral)

### Mistral
- `MISTRAL_API_KEY`: Ihr Mistral API Schl√ºssel
- `MISTRAL_BASE_URL`: API Basis-URL (Standard: https://api.mistral.ai/v1)
- `MISTRAL_MODEL`: Modell (Standard: mistral-small-latest)

### Ollama Cloud
- `OLLAMA_API_KEY`: Ihr Ollama Cloud API Schl√ºssel
- `OLLAMA_MODEL`: Modell (Standard: llama3.2:3b)

### Ollama Local
- `OLLAMA_BASE_URL`: URL zu lokaler Ollama-Instanz (Standard: http://host.docker.internal:11434/v1)
- `OLLAMA_MODEL`: Modell (Empfohlen: gemma3:latest)

### Sicherheit
- `ALLOWED_ORIGINS`: Erlaubte CORS Origins (Standard: http://localhost,http://localhost:80)
- `RATE_LIMIT_PER_IP`: Anfragen pro Stunde pro IP (Standard: 10)
- `GLOBAL_DAILY_LIMIT`: Max Anfragen pro Tag (Standard: 1000)
- `MAX_STORY_LENGTH`: Max Story-L√§nge in Minuten (Standard: 15)
- `MAX_DAILY_COST`: Max Kosten pro Tag in Euro (Standard: 5.0)

**Wichtig**: Die `.env` Datei ist in `.gitignore` und wird nicht ins Repository committed!

## üîí Sicherheit

Die API ist durch mehrere Sicherheitsebenen gesch√ºtzt:

1. **Backend nur auf localhost**: Das FastAPI-Backend lauscht nur auf `127.0.0.1:8000` und ist von au√üen nicht direkt erreichbar
2. **Nginx als einziger Zugangspunkt**: Nur Nginx kann auf das Backend zugreifen und fungiert als Reverse Proxy
3. **CORS-Einschr√§nkung**: Nur erlaubte Origins (konfiguriert via `ALLOWED_ORIGINS`) k√∂nnen API-Requests durchf√ºhren

### F√ºr Produktion

In der Produktion sollten Sie `ALLOWED_ORIGINS` auf Ihre echte Domain(s) setzen:

```bash
# In .env
ALLOWED_ORIGINS=https://mairchen.de,https://www.mairchen.de
```

Dies verhindert, dass andere Websites Ihre API nutzen k√∂nnen, auch wenn sie die URL kennen. Das Frontend kann weiterhin von Client-Ger√§ten auf die API zugreifen, da die Requests √ºber Ihren Server laufen.

## üåê Netzwerk-Zugriff

Die App ist von anderen Ger√§ten im Netzwerk erreichbar:
1. Finde die IP-Adresse deines Computers: `ifconfig` (Mac/Linux) oder `ipconfig` (Windows)
2. √ñffne auf einem anderen Ger√§t: `http://<deine-ip>`

Das Frontend nutzt automatisch die richtige URL f√ºr API-Requests.

## üê≥ Deployment

### Mit GitHub Container Registry
Der Container wird automatisch bei jedem Push auf `main` gebaut und in die GitHub Container Registry gepusht.

**Container direkt von GitHub pullen:**
```bash
docker pull ghcr.io/sebastiansucker/mairchen:latest
docker run -d -p 80:80 \
  -e MISTRAL_API_KEY=your-key \
  -e MISTRAL_BASE_URL=https://api.mistral.ai/v1 \
  -e MISTRAL_MODEL=mistral-small-latest \
  --name mairchen-app \
  ghcr.io/sebastiansucker/mairchen:latest
```

**Mit Docker Compose und GitHub Registry:**
```yaml
services:
  app:
    image: ghcr.io/sebastiansucker/mairchen:latest
    container_name: mairchen-app
    ports:
      - "80:80"
    environment:
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
      - MISTRAL_BASE_URL=${MISTRAL_BASE_URL:-https://api.mistral.ai/v1}
      - MISTRAL_MODEL=${MISTRAL_MODEL:-mistral-small-latest}
    restart: unless-stopped
```

### Manuelles Deployment (Lokaler Build)
```bash
# Auf dem Server
git clone git@github.com:sebastiansucker/mAIrchen.git
cd mAIrchen
cp .env.example .env
# .env bearbeiten und API-Key eintragen
docker-compose --env-file .env -f docker/docker-compose.yml up -d
```