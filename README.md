# üìö mAIrchen - Geschichten f√ºr Kinder

Eine M√§rchen-Schreib-App f√ºr Grundschulkinder (Klasse 1-4), die personalisierte Geschichten mit W√∂rtern aus dem Grundwortschatz generiert.

## ‚ú® Funktionen

- **Personalisierte Geschichten**: Der Nutzer gibt Thema, Personen/Tiere, Ort und Stimmung ein
- **Zufalls-Generator**: Automatische Vorschl√§ge f√ºr alle Parameter
- **Grundwortschatz-Integration**: Geschichten enthalten W√∂rter aus dem Grundwortschatz der Klassen 1-4
- **Buchlayout**: Ansprechende Darstellung im Buchformat mit vergilbtem Papier-Look
- **Seitenbl√§tter-Animation**: Geschichten erscheinen mit einer 3D-Bl√§tter-Animation
- **Flexible AI-Provider**: Unterst√ºtzt alle OpenAI-kompatiblen APIs (OpenAI, Mistral, Together AI, etc.) plus Ollama (Cloud & Local)
- **Missbrauchsschutz**: Rate Limiting, Cost Control und Request-Validierung ohne Login
- **Single-Container**: Frontend und Backend in einem Container f√ºr einfaches Deployment

## üöÄ Installation

### Voraussetzungen

- Docker und Docker Compose
- AI Provider: OpenAI-compatible API Key (OpenAI, Mistral, Together AI, etc.) ODER Ollama (Cloud/Local)

### Setup

1. Repository klonen und in das Verzeichnis wechseln:
```bash
cd mAIrchen
```

2. Umgebungsvariablen konfigurieren:
```bash
cp .env.example .env
```

3. `.env`-Datei bearbeiten und AI Provider konfigurieren:

**Option A: OpenAI-compatible API** (Standard - Mistral AI als Default)
```env
AI_PROVIDER=openai
OPENAI_API_KEY=your-api-key
OPENAI_BASE_URL=https://api.mistral.ai/v1  # Default: Mistral AI
OPENAI_MODEL=mistral-small-latest  # Default: mistral-small-latest
```

**Unterst√ºtzte OpenAI-kompatible Provider:**
- **Mistral AI** (mistral-small-latest, mistral-large-latest) - **Default**
- OpenAI (gpt-4o-mini, gpt-4o, etc.)
- Together AI
- Anyscale Endpoints
- OpenRouter
- Azure OpenAI
- Jeder andere Provider mit OpenAI-kompatibler API

**Option B: Ollama Cloud**
```env
AI_PROVIDER=ollama-cloud
OLLAMA_API_KEY=your-ollama-api-key
OLLAMA_MODEL=llama3.2:3b
```

**Option C: Ollama Lokal (kostenlos)** ‚≠ê Empfohlen f√ºr Entwicklung
```env
AI_PROVIDER=ollama-local
OLLAMA_BASE_URL=http://host.docker.internal:11434/v1
OLLAMA_MODEL=gemma3:latest  # Beste Balance: Schnell & gute Qualit√§t
```

**Empfohlene Ollama-Modelle f√ºr Kindergeschichten:**
- `gemma3:latest` - üèÜ Beste Wahl (7.5s, sehr gute Qualit√§t)
- `gemma3n:latest` - Etwas langsamer, exzellente Qualit√§t (14.9s)
- `llama3.2:3b` - Klein und schnell, gute Basisqualit√§t
- `gemma3:27b` - Beste Qualit√§t, aber langsam (38s)

4. Container bauen & starten:
```bash
docker-compose up --build -d
```

Die App ist nun verf√ºgbar unter:
- **Frontend**: http://localhost
- **API Info**: http://localhost/api (zeigt aktiven Provider & Modell)
- **API Stats**: http://localhost/api/stats (Monitoring)
- **Health Check**: http://localhost/health

## üìä API Endpoints

Alle API-Endpoints sind √ºber Port 80 (HTTP) erreichbar:

### GET /api
Zeigt aktiven AI-Provider und Modell:
```bash
curl http://localhost/api
```
```json
{
  "message": "mAIrchen API - M√§rchen f√ºr Kinder",
  "ai_provider": "openai",
  "model": "mistral-small-latest"
}
```

### GET /api/stats
Monitoring und Nutzungsstatistiken:
```bash
curl http://localhost/api/stats
```
```json
{
  "global_requests_today": 42,
  "global_limit": 1000,
  "estimated_cost_today": 0.0,
  "daily_budget": 5.0,
  "budget_remaining": 5.0,
  "rate_limit_per_ip": 10,
  "active_ips": 8
}
```

### POST /api/generate-story
Generiert eine personalisierte Geschichte:
```bash
curl -X POST http://localhost/api/generate-story \
  -H "Content-Type: application/json" \
  -d '{
    "thema": "Freundschaft",
    "personen_tiere": "Ein kleiner Igel",
    "ort": "im Wald",
    "stimmung": "herzlich",
    "laenge": 3,
    "klassenstufe": "34"
  }'
```

### GET /api/random
Zuf√§llige Vorschl√§ge f√ºr alle Parameter:
```bash
curl http://localhost/api/random
```

## üîí Sicherheit & Missbrauchsschutz

Die API ist √ºber Port 80 erreichbar, aber durch mehrere Schutzebenen gesichert:

### Aktive Schutzma√ünahmen:
- **Rate Limiting**: 10 Anfragen/h pro IP, 1000/Tag global (IP-basiert)
- **Request-Validierung**: Max 15 Min Story-L√§nge, 200 Zeichen pro Feld
- **Cost Control**: Max 5‚Ç¨/Tag Budget mit automatischem Stop
- **CORS-Schutz**: Nur erlaubte Origins (konfigurierbar)
- **Nginx Reverse Proxy**: Backend nur intern erreichbar (127.0.0.1:8000)
- Keine User-Accounts erforderlich - Privacy-Friendly!

### Wie es funktioniert:
1. Backend l√§uft nur auf `127.0.0.1:8000` (nicht von au√üen erreichbar)
2. Nginx auf Port 80 leitet Anfragen an Backend weiter
3. Rate Limiting pr√ºft jede Anfrage anhand der IP-Adresse
4. CORS verhindert Zugriff von fremden Websites

Details: [SECURITY.md](SECURITY.md)

## üèóÔ∏è Architektur

### Single Container Setup
Frontend und Backend laufen in einem Docker-Container:
- **Nginx** serviert das Frontend (Port 80)
- **Go Backend** (Gin Framework) l√§uft auf Port 8000 (intern)
- Nginx fungiert als Reverse Proxy f√ºr `/api/*` Requests

### Backend (Go + Gin)
- Go-basierte REST API mit Gin Framework
- OpenAI-kompatibler Client (go-openai)
- Modular aufgebaute Package-Struktur:
  - `pkg/config` - Provider-Konfiguration
  - `pkg/data` - Eingebettete Grundwortschatz-Daten
  - `pkg/prompt` - Prompt-Generierung
  - `pkg/story` - Story-Generierung
  - `pkg/analysis` - Grundwortschatz-Analyse
- Flexible AI-Provider-Konfiguration √ºber Umgebungsvariablen
- Rate Limiting & Cost Tracking
- Endpunkte:
  - `GET /` - API Info (Provider & Modell)
  - `GET /api/random` - Zuf√§llige Vorschl√§ge
  - `POST /api/generate-story` - Geschichte generieren
  - `GET /api/stats` - Monitoring & Statistiken
  - `GET /health` - Health Check

### Frontend
- Vanilla HTML/CSS/JavaScript
- Responsive Design
- 3D-Seitenbl√§tter-Animation
- Buchlayout mit vergilbtem Papier-Effekt
- Automatische API-URL-Erkennung (funktioniert im Netzwerk)

### Projektstruktur
```
mAIrchen/
‚îú‚îÄ‚îÄ .env.example          # Umgebungsvariablen Template
‚îú‚îÄ‚îÄ .gitignore           # Git Ignore Datei
‚îú‚îÄ‚îÄ README.md            # Diese Datei
‚îú‚îÄ‚îÄ docker-compose.yml   # Container Orchestrierung
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ main.go          # Go Backend (Gin)
‚îÇ   ‚îú‚îÄ‚îÄ go.mod, go.sum   # Go Dependencies
‚îÇ   ‚îî‚îÄ‚îÄ pkg/
‚îÇ       ‚îú‚îÄ‚îÄ config/      # Provider-Konfiguration
‚îÇ       ‚îú‚îÄ‚îÄ data/        # Grundwortschatz (embedded gws.md)
‚îÇ       ‚îú‚îÄ‚îÄ prompt/      # Prompt-Generierung
‚îÇ       ‚îú‚îÄ‚îÄ story/       # Story-Generierung
‚îÇ       ‚îî‚îÄ‚îÄ analysis/    # Grundwortschatz-Analyse
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ index.html      # Haupt-HTML
‚îÇ   ‚îú‚îÄ‚îÄ styles.css      # Styling & Animationen
‚îÇ   ‚îú‚îÄ‚îÄ app.js          # JavaScript Logik
‚îÇ   ‚îî‚îÄ‚îÄ nginx.conf      # Frontend Nginx Config
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îî‚îÄ‚îÄ model_comparison.go  # Benchmark-Tool f√ºr Model-Vergleiche
‚îî‚îÄ‚îÄ docker/
    ‚îú‚îÄ‚îÄ Dockerfile              # Multi-Stage Build (Go + Nginx)
    ‚îú‚îÄ‚îÄ nginx-combined.conf     # Nginx Konfiguration
    ‚îî‚îÄ‚îÄ start-go.sh             # Container Start-Script
```

## üéØ Verwendung

1. App im Browser √∂ffnen (http://localhost)
2. Eingabefelder ausf√ºllen:
   - Thema (z.B. "Freundschaft")
   - Personen/Tiere (z.B. "Ein kleiner Hase")
## üõ†Ô∏è Entwicklung

### Backend lokal starten
```bash
cd backend
# Dependencies installieren
go mod download

# Umgebungsvariablen setzen
export AI_PROVIDER=ollama-cloud
export OLLAMA_API_KEY=your-key
export OLLAMA_MODEL=ministral-3:8b-cloud

# Backend starten
go run main.go
```

### Tests ausf√ºhren
```bash
cd backend
# Alle Tests
go test ./...

# Mit Coverage
go test ./... -cover

# Verbose Output
go test ./... -v
```

### Linting
```bash
cd backend
golangci-lint run
```

### Frontend lokal testen
Das Frontend ben√∂tigt das Backend auf Port 8000:
```bash
cd frontend
python -m http.server 8080
```
Dann im Browser: http://localhost:8080

### Container neu bauen nach √Ñnderungen
```bash
docker-compose up --build -d
```

### Model Comparison Tool
Vergleicht verschiedene AI-Modelle f√ºr Kindergeschichten:
```bash
cd tools
go run model_comparison.go
```n im Browser: http://localhost:8080

### Container neu bauen nach √Ñnderungen
```bash
docker-compose --env-file .env -f docker/docker-compose.yml build
docker-compose --env-file .env -f docker/docker-compose.yml up -d
```

## üìù API Endpunkte

### Zuf√§llige Vorschl√§ge
```http
GET /api/random
```

### Geschichte generieren
```http
POST /api/generate-story
Content-Type: application/json

{
  "thema": "Abenteuer",
  "personen_tiere": "Ein mutiger Fuchs",
  "ort": "im Wald",
  "stimmung": "spannend"
}
```

## üîß Konfiguration

Umgebungsvariablen in `.env`:

### AI Provider
- `AI_PROVIDER`: `openai`, `ollama-cloud` oder `ollama-local` (Standard: openai)

### OpenAI-compatible API
- `OPENAI_API_KEY`: Ihr API Schl√ºssel
- `OPENAI_BASE_URL`: API Basis-URL (Standard: https://api.openai.com/v1)
- `OPENAI_MODEL`: Modell (Standard: gpt-4o-mini)

**Beispiele f√ºr verschiedene Provider:**
```env
# Mistral AI (Default)
OPENAI_BASE_URL=https://api.mistral.ai/v1
OPENAI_MODEL=mistral-small-latest

# OpenAI
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini

# Together AI
OPENAI_BASE_URL=https://api.together.xyz/v1
OPENAI_MODEL=meta-llama/Llama-3.2-3B-Instruct-Turbo

# OpenRouter
OPENAI_BASE_URL=https://openrouter.ai/api/v1
OPENAI_MODEL=meta-llama/llama-3.2-3b-instruct
```

### Ollama Cloud
- `OLLAMA_API_KEY`: Ihr Ollama Cloud API Schl√ºssel
- `OLLAMA_MODEL`: Modell (Standard: llama3.2:3b)

### Ollama Local
- `OLLAMA_BASE_URL`: URL zu lokaler Ollama-Instanz (Standard: http://host.docker.internal:11434/v1)
- `OLLAMA_MODEL`: Modell (Empfohlen: gemma3:latest)

### Sicherheit
- `ALLOWED_ORIGINS`: Erlaubte CORS Origins (Standard: http://localhost,http://localhost:80)
- `RATE_LIMIT_PER_IP`: Anfragen pro Stunde pro IP (Standard: 10)
- `GLOBAL_DAILY_LIMIT`: Max Anfragen pro Tag (Standard: 1000)
- `MAX_STORY_LENGTH`: Max Story-L√§nge in Minuten (Standard: 15)
- `MAX_DAILY_COST`: Max Kosten pro Tag in Euro (Standard: 5.0)

**Wichtig**: Die `.env` Datei ist in `.gitignore` und wird nicht ins Repository committed!

## üîí Sicherheit

Die API ist durch mehrere Sicherheitsebenen gesch√ºtzt:

1. **Backend nur auf localhost**: Das FastAPI-Backend lauscht nur auf `127.0.0.1:8000` und ist von au√üen nicht direkt erreichbar
2. **Nginx als einziger Zugangspunkt**: Nur Nginx kann auf das Backend zugreifen und fungiert als Reverse Proxy
3. **CORS-Einschr√§nkung**: Nur erlaubte Origins (konfiguriert via `ALLOWED_ORIGINS`) k√∂nnen API-Requests durchf√ºhren

### F√ºr Produktion

In der Produktion sollten Sie `ALLOWED_ORIGINS` auf Ihre echte Domain(s) setzen:

```bash
# In .env
ALLOWED_ORIGINS=https://mairchen.de,https://www.mairchen.de
```

Dies verhindert, dass andere Websites Ihre API nutzen k√∂nnen, auch wenn sie die URL kennen. Das Frontend kann weiterhin von Client-Ger√§ten auf die API zugreifen, da die Requests √ºber Ihren Server laufen.

## üåê Netzwerk-Zugriff

Die App ist von anderen Ger√§ten im Netzwerk erreichbar:
1. Finde die IP-Adresse deines Computers: `ifconfig` (Mac/Linux) oder `ipconfig` (Windows)
2. √ñffne auf einem anderen Ger√§t: `http://<deine-ip>`

Das Frontend nutzt automatisch die richtige URL f√ºr API-Requests.

### Manuelles Deployment (Lokaler Build)
```bash
# Auf dem Server
git clone git@github.com:sebastiansucker/mAIrchen.git
cd mAIrchen
cp .env.example .env
# .env bearbeiten und API-Key eintragen
docker-compose up --build -d
```

## üß™ Testing & CI/CD

Das Projekt nutzt GitHub Actions f√ºr automatisierte Tests und Builds:

### Automated Testing
- **golangci-lint**: L√§uft bei jedem Pull Request und Push auf `main`
- **Unit Tests**: Alle Packages haben vollst√§ndige Test-Coverage
- **Docker Build**: Automatischer Build und Push zu GitHub Container Registry

### Lokale Tests
```bash
# Backend Tests
cd backend
go test ./pkg/... -v

# Mit Coverage Report
go test ./pkg/... -cover -coverprofile=coverage.out
go tool cover -html=coverage.out
```ker run -d -p 80:80 \
  -e MISTRAL_API_KEY=your-key \
  -e MISTRAL_BASE_URL=https://api.mistral.ai/v1 \
  -e MISTRAL_MODEL=mistral-small-latest \
  --name mairchen-app \
  ghcr.io/sebastiansucker/mairchen:latest
```

**Mit Docker Compose und GitHub Registry:**
```yaml
services:
  app:
    image: ghcr.io/sebastiansucker/mairchen:latest
    container_name: mairchen-app
    ports:
      - "80:80"
    environment:
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
      - MISTRAL_BASE_URL=${MISTRAL_BASE_URL:-https://api.mistral.ai/v1}
      - MISTRAL_MODEL=${MISTRAL_MODEL:-mistral-small-latest}
    restart: unless-stopped
```

### Manuelles Deployment (Lokaler Build)
```bash
# Auf dem Server
git clone git@github.com:sebastiansucker/mAIrchen.git
cd mAIrchen
cp .env.example .env
# .env bearbeiten und API-Key eintragen
docker-compose --env-file .env -f docker/docker-compose.yml up -d
```